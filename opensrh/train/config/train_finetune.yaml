infra:
    log_dir: ./ # where all the experiments are
    exp_name: opensrh_finetune # create a subdirectory for one experiment
    comment: dev # can use this to customize for each experiment
    seed: 1000
data:
    db_root:  /path/to/opensrh/ 
    train_augmentation:
    - which: random_horiz_flip
      params: {}
    - which: random_vert_flip
      params: {}
    valid_augmentation: []
    rand_aug_prob: 0.5
    balance_patch_per_class: true
model:
    backbone: resnet50
    mlp_hidden: []
training:
    batch_size: 56 
    num_epochs: 20
    optimizer: adamw # [sgd, adam, adamw]
    learn_rate: 0.01
    scheduler:
        which: step_lr
        params:
            step_size: 1
            step_unit: epoch # epoch or iter
            gamma: 0.5
    finetune_mode: linear
    backbone_checkpoint: /path/to/checkpoint.pt
